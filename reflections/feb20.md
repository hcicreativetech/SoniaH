# February 20th

## *Evaluation Strategies for HCI Toolkit Research*
by David Ledo, Steven Houben, Jo Vermeulen, Nicolai Marquadt, Lora Ohelberg, Saul Greenberg

> four types of evaluation strategies: (1) demonstration, (2)
usage, (3) technical benchmarks, and (4) heuristics, 1.

Definition: Toolkit:
> Within HCI literature, the term ‘toolkit’ is widely used to
describe various types of software, hardware, design and
conceptual frameworks. Toolkit research falls into a category
of constructive research, which Oulasvirta and Hornbæk
define as “producing understanding about the construction of
an interactive artefact for some purpose in human use of
computing” [83]. They specify that constructive research is
driven by the absence of a (full) known solution or resources
to implement and deploy that solution, 1

> We extend Greenberg’s original definition [30] to define
toolkits as generative platforms designed to create new interactive artifacts, provide easy access to complex algorithms,
enable fast prototyping of software and hardware interfaces,
and/or enable creative exploration of design spaces. Hence,
toolkits present users with a programming or configuration
environment consisting of many defined permutable building
blocks, structures, or primitives, with a sequencing of logical
or design flow affording a path of least resistance, 2.

Why do HCI Researchers Build Toolkits?

- Reducing authoring time & complexity
- Creating paths of least resistance
- Empowering new audiences
- Integrating current practices and infrastructures
- Enabling replication and creative exploration

(Are there value judgements attached to these in terms of which ones are preferable?)

> As
summarized by Olsen [82] in his reflective paper on evaluat ing systems research: “simple metrics can produce simplistic
progress that is not necessarily meaningful.” The central
question is thus: what is an evaluation? And, how do we reflect and evaluate such complex toolkit research?, 2 - 3.

Type 1: Demonstration

Type 2: Usage

Type 3: Technical Performance

Type 4: Heuristics


This reading provided an insightful for overview of research evaluation strategies employed to asesss HCI toolkits. The takeaways were insightful in how to avoid the shortcomings of each approach and strengthen the overall assessment. At the same time, I was struck by the lack of standardization and divergent goals for many of the examples discussed. Every paper in addition to introducing a framework or new body of knowledge ought to provide a motivation and explanation for its application or evaluation including justifying the evaluation methods applied. They are often context driven and specific to each example but most fields have standard practices that help in establishing a consistent baseline for comparison and also to help establish goals for future progress. I was most interested in how the fourth approach, heuristics, for instance the degree to which a toolkit matches its intended functionality could be used to bolster or perhaps structure and guide toolkit development. The lack of consistent structured design goals also leads to a less clear target for a community or for other communities to borrow from in developing creative tech tools. Repeatedly, ideas in this course have evoked the idea of learnability. This seems like a possibilities general starting point for assessing toolkit design and efficacy but also establishing a target for what creative tech tools should aim to achieve.
